<div align="center">

# ğŸ¯ Awesome SAM Foundation Models
### *Comprehensive Resource Collection for Segment Anything*

[![Awesome](https://awesome.re/badge-flat2.svg)](https://awesome.re)
[![GitHub stars](https://img.shields.io/github/stars/umitkacar/SAM-Foundation-Models?style=social&label=Star)](https://github.com/umitkacar/SAM-Foundation-Models/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/umitkacar/SAM-Foundation-Models?style=social&label=Fork)](https://github.com/umitkacar/SAM-Foundation-Models/network/members)
[![GitHub watchers](https://img.shields.io/github/watchers/umitkacar/SAM-Foundation-Models?style=social&label=Watch)](https://github.com/umitkacar/SAM-Foundation-Models/watchers)

<img src="https://img.shields.io/badge/SAM-2.1-blue?style=for-the-badge&logo=meta&logoColor=white" alt="SAM 2.1"/>
<img src="https://img.shields.io/badge/Updated-January_2025-green?style=for-the-badge&logo=github&logoColor=white" alt="Updated"/>
<img src="https://img.shields.io/badge/Papers-100+-red?style=for-the-badge&logo=arxiv&logoColor=white" alt="Papers"/>
<img src="https://img.shields.io/badge/License-Apache_2.0-yellow?style=for-the-badge&logo=apache&logoColor=white" alt="License"/>
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=for-the-badge)](http://makeapullrequest.com)
[![Made with Love](https://img.shields.io/badge/Made%20with-â¤ï¸-red.svg?style=for-the-badge)](https://github.com/umitkacar/SAM-Foundation-Models)

### ğŸš€ Curated collection of SAM resources, implementations, optimizations & applications
#### Updated with 2024-2025 SOTA developments

---

[ğŸ“– Explore](#-table-of-contents) â€¢ [ğŸ”¥ Latest Updates](#-official-models--latest-updates) â€¢ [ğŸ’¡ Applications](#-domain-specific-applications) â€¢ [ğŸ› ï¸ Tools](#-production-deployment--tools) â€¢ [ğŸ“š Learn](#-educational-resources--tutorials)

---

</div>

## ğŸŒŸ Highlights

<table>
<tr>
<td width="33%" align="center">
<h3>âš¡ 6x Faster</h3>
<p>SAM 2 processes images<br/>6x faster than original SAM</p>
</td>
<td width="33%" align="center">
<h3>ğŸ¯ 3x Efficient</h3>
<p>Requires 3x fewer<br/>user interactions</p>
</td>
<td width="33%" align="center">
<h3>ğŸ“± 30+ FPS</h3>
<p>Real-time on mobile<br/>devices (EdgeSAM)</p>
</td>
</tr>
<tr>
<td width="33%" align="center">
<h3>ğŸ† ICLR 2025</h3>
<p>Best Paper<br/>Honorable Mention</p>
</td>
<td width="33%" align="center">
<h3>ğŸ¬ 44 FPS</h3>
<p>Real-time video<br/>segmentation</p>
</td>
<td width="33%" align="center">
<h3>ğŸ¥ 26+ Tasks</h3>
<p>Medical imaging<br/>benchmarks</p>
</td>
</tr>
</table>

---

## ğŸ“‘ Table of Contents

<details open>
<summary><b>Click to expand/collapse</b></summary>

- [ğŸ”¥ Official Models & Latest Updates](#-official-models--latest-updates)
- [ğŸ¬ SAM 2 & Video Segmentation](#-sam-2--video-segmentation)
- [âš¡ Optimization & Mobile Deployment](#-optimization--mobile-deployment)
- [ğŸ“š Academic Research & Surveys](#-academic-research--surveys)
- [ğŸ’¡ Domain-Specific Applications](#-domain-specific-applications)
  - [ğŸ¥ Medical Imaging](#-medical-imaging-2024-2025)
  - [ğŸ›°ï¸ Remote Sensing & Agriculture](#ï¸-remote-sensing--agriculture)
  - [ğŸš— Autonomous Driving & Robotics](#-autonomous-driving--robotics)
  - [ğŸ­ Industrial Quality Control](#-industrial-quality-control)
- [ğŸ“ Training & Fine-Tuning](#-training--fine-tuning)
- [ğŸ› ï¸ Production Deployment & Tools](#ï¸-production-deployment--tools)
- [ğŸ”§ SAM Extensions & Variants](#-sam-extensions--variants)
- [ğŸ’» Implementation Libraries](#-implementation-libraries)
- [ğŸ“Š Datasets & Benchmarks](#-datasets--benchmarks)
- [ğŸ“– Educational Resources](#-educational-resources--tutorials)
- [ğŸ¯ Key Insights & Trends](#-key-insights--trends-2024-2025)

</details>

---

## ğŸ”¥ Official Models & Latest Updates

<div align="center">

### ğŸ¯ Meta's Official SAM Family

</div>

<details open>
<summary><b>ğŸŒŸ SAM 2.1 - Latest Release (September 2024)</b></summary>

<br/>

| Feature | Performance | Status |
|---------|-------------|--------|
| ğŸš€ Speed | **6x faster** than SAM 1 | âœ… Released |
| ğŸ¯ Efficiency | **3x fewer** interactions | âœ… Stable |
| ğŸ¬ Video FPS | **44 FPS** real-time | âœ… Production |
| ğŸ“œ License | Apache 2.0 | âœ… Open Source |
| ğŸ† Recognition | ICLR 2025 Best Paper | âœ… Awarded |

#### ğŸ“¥ Official Resources

```bash
# Install SAM 2
pip install segment-anything-2

# Quick Start
from sam2 import SAM2Model
model = SAM2Model.from_pretrained("facebook/sam2-hiera-large")
```

**Links:**
- ğŸ“¦ [GitHub Repository](https://github.com/facebookresearch/sam2) ![Stars](https://img.shields.io/github/stars/facebookresearch/sam2?style=social)
- ğŸ“ [Meta AI Blog](https://ai.meta.com/blog/segment-anything-2/)
- ğŸ“„ [Research Paper - ArXiv:2408.00714](https://arxiv.org/abs/2408.00714)
- ğŸ¤— [HuggingFace Hub](https://huggingface.co/facebook/sam2-hiera-large) - 182K+ downloads/month
- â˜ï¸ [AWS SageMaker JumpStart](https://ai.meta.com/blog/segment-anything-2/) - Feb 2025

</details>

<details>
<summary><b>ğŸ¨ Original SAM (April 2023)</b></summary>

<br/>

- ğŸ  [Official Website](https://segment-anything.com/)
- ğŸ“¦ [GitHub Repository](https://github.com/facebookresearch/segment-anything) ![Stars](https://img.shields.io/github/stars/facebookresearch/segment-anything?style=social)
- ğŸ“„ [Paper - ArXiv:2304.02643](https://arxiv.org/abs/2304.02643)
- ğŸ¯ [Papers with Code](https://paperswithcode.com/paper/segment-anything)

</details>

<details>
<summary><b>ğŸ”— Integration Platforms</b></summary>

<br/>

| Platform | Features | Link |
|----------|----------|------|
| ğŸ¤– **Ultralytics** | Production-ready YOLO integration | [Docs](https://docs.ultralytics.com/models/sam-2/) |
| ğŸ¤— **HuggingFace** | Model hub, Transformers support | [Hub](https://huggingface.co/facebook/sam2-hiera-large) |
| â˜ï¸ **SageMaker** | AWS deployment ready | [JumpStart](https://ai.meta.com/blog/segment-anything-2/) |

</details>

---

## ğŸ¬ SAM 2 & Video Segmentation

### ğŸŒŸ Advanced Video Segmentation (2024-2025)

<table>
<tr>
<td width="50%">

#### ğŸ† [SAM2Long](https://github.com/Mark12Ding/SAM2Long)
**ICCV 2025** | ![Stars](https://img.shields.io/github/stars/Mark12Ding/SAM2Long?style=social)

- ğŸ¯ Training-free memory tree
- ğŸ”„ Long video segmentation
- ğŸ­ Handles occlusion/reappearance
- ğŸ“Š [Project Page](https://mark12ding.github.io/project/SAM2Long/)

</td>
<td width="50%">

#### ğŸ”¥ [Grounded-SAM-2](https://github.com/IDEA-Research/Grounded-SAM-2)
**Multi-modal Integration** | ![Stars](https://img.shields.io/github/stars/IDEA-Research/Grounded-SAM-2?style=social)

- ğŸ¤ Grounding DINO + Florence-2 + SAM 2
- ğŸ¯ Detect + Segment + Track
- ğŸ¬ Video object tracking
- ğŸŒ HuggingFace ready

</td>
</tr>
<tr>
<td width="50%">

#### ğŸ¤ [AL-Ref-SAM2](https://github.com/appletea233/AL-Ref-SAM2)
**AAAI 2025** | ![Stars](https://img.shields.io/github/stars/appletea233/AL-Ref-SAM2?style=social)

- ğŸ”Š Audio-Language-Referenced VOS
- ğŸ§  GPT temporal-spatial reasoning
- ğŸ¯ Training-free paradigm
- ğŸš€ State-of-the-art results

</td>
<td width="50%">

#### ğŸ¥ [Surgical SAM 2](https://github.com/jinlab-imvr/Surgical-SAM-2)
**NeurIPS 2024** | ![Stars](https://img.shields.io/github/stars/jinlab-imvr/Surgical-SAM-2?style=social)

- âš¡ **86 FPS** real-time
- ğŸ¥ Medical surgery segmentation
- ğŸ’ª 3x faster performance
- ğŸ“± Resource-constrained ready

</td>
</tr>
</table>

### ğŸ“¼ Classic Video & Tracking

- ğŸ¯ [Segment-and-Track-Anything](https://github.com/z-x-yang/Segment-and-Track-Anything) ![Stars](https://img.shields.io/github/stars/z-x-yang/Segment-and-Track-Anything?style=social)
- âš¡ [RAP-SAM](https://github.com/xushilin1/rap-sam) - Real-Time All-Purpose on Video

---

## âš¡ Optimization & Mobile Deployment

<div align="center">

### ğŸ“± Edge & Mobile Champions

</div>

<table>
<tr>
<th width="25%">Model</th>
<th width="15%">Speed</th>
<th width="15%">Size</th>
<th width="15%">Device</th>
<th width="30%">Highlights</th>
</tr>
<tr>
<td><b>ğŸš€ EdgeSAM</b></td>
<td><b>30+ FPS</b></td>
<td>-</td>
<td>iPhone 14</td>
<td>First mobile SAM @ 30+ FPS</td>
</tr>
<tr>
<td><b>âš¡ EfficientSAM</b></td>
<td>10-20 img/s</td>
<td><b>9.8M</b></td>
<td>Edge</td>
<td>Best accuracy-efficiency trade-off</td>
</tr>
<tr>
<td><b>ğŸ“± MobileSAM</b></td>
<td><b>40x</b></td>
<td><b>60x smaller</b></td>
<td>Mobile</td>
<td>Lightweight variant</td>
</tr>
<tr>
<td><b>ğŸƒ FastSAM</b></td>
<td><b>~100 img/s</b></td>
<td>68M</td>
<td>GPU</td>
<td>Maximum throughput</td>
</tr>
</table>

<details>
<summary><b>ğŸ“Š Performance Comparison: EfficientSAM vs FastSAM</b></summary>

<br/>

| Metric | EfficientSAM-Ti | EfficientSAM-S | FastSAM |
|--------|-----------------|----------------|---------|
| **COCO AP** | 45.0 (+4.1) | - (+6.5) | 37.0 |
| **LVIS AP** | - (+5.3) | - (+7.8) | - |
| **Params** | **9.8M** | - | 68M |
| **Speed** | 10-20 img/s | 10-20 img/s | ~100 img/s |

**Winner:** ğŸ† EfficientSAM for accuracy, ğŸ† FastSAM for speed

</details>

### ğŸ› ï¸ Optimization Resources

<details>
<summary><b>Click to see optimization tools & papers</b></summary>

#### ğŸ“š Papers & Surveys
- ğŸ“„ [ArXiv: On Efficient Variants of SAM](https://arxiv.org/html/2410.04960v1) (2024)
- ğŸ” [Awesome-Efficient-Segment-Anything](https://github.com/czg1225/Awesome-Efficient-Segment-Anything)

#### ğŸ”— Repositories

| Model | Link | Special Feature |
|-------|------|-----------------|
| ğŸš€ **EdgeSAM** | [GitHub](https://github.com/chongzhou96/EdgeSAM) â€¢ [Paper](https://arxiv.org/abs/2312.06660) | CNN-based, iPhone ready |
| âš¡ **EfficientSAM** | [GitHub](https://github.com/yformer/EfficientSAM) â€¢ [Site](https://yformer.github.io/efficient-sam/) | Best AP/params ratio |
| ğŸ“± **MobileSAM** | [GitHub](https://github.com/ChaoningZhang/MobileSAM) | 60x compression |
| ğŸƒ **FastSAM** | [GitHub](https://github.com/CASIA-IVA-Lab/FastSAM) â€¢ [Docs](https://docs.ultralytics.com/models/fast-sam/) | Prompt-free |
| ğŸ”¬ **TinySAM** | [GitHub](https://github.com/xinghaochen/TinySAM) | Ultra-compact |
| ğŸ’ **HQ-SAM** | [GitHub](https://github.com/SysCV/sam-hq) | NeurIPS 2023, Quality++ |

</details>

---

## ğŸ“š Academic Research & Surveys

<div align="center">

### ğŸ“– Comprehensive Surveys (2024-2025)

</div>

<details open>
<summary><b>ğŸŒŸ Must-Read Surveys</b></summary>

<br/>

#### ğŸ“˜ [A Survey on SAM: Vision Foundation Model Meets Prompt Engineering](https://arxiv.org/abs/2306.06211)
**ArXiv:2306.06211** â€¢ Updated: Oct 2024 â€¢ ğŸ† Most Comprehensive

```
ğŸ“… Coverage: April 2023 - September 2024
ğŸ¯ Topics: SAM & SAM 2, Prompt Engineering
ğŸ“Š Papers: 200+ analyzed
â­ Rating: â­â­â­â­â­
```

#### ğŸ“— [A Comprehensive Survey on SAM for Vision and Beyond](https://arxiv.org/abs/2305.08196)
**ArXiv:2305.08196** â€¢ Foundation model analysis

```
ğŸ¯ Topics: Computer Vision Applications
ğŸ”¬ Depth: Technical Deep Dive
ğŸ“Š Applications: Multiple Domains
```

#### ğŸ“™ [Segment Anything for Video: Past to Future](https://arxiv.org/abs/2507.22792)
**ArXiv:2507.22792** â€¢ 2025 â€¢ Video-specific review

```
ğŸ¬ Focus: Video Object Segmentation & Tracking
â° Timeline: Past â†’ Present â†’ Future
ğŸ¯ Comprehensive VOST analysis
```

</details>

<details>
<summary><b>ğŸ—‚ï¸ Curated Collections</b></summary>

<br/>

| Repository | Description | Stars |
|------------|-------------|-------|
| ğŸ“š [Awesome-Segment-Anything](https://github.com/liliu-avril/Awesome-Segment-Anything) | First comprehensive survey | ![Stars](https://img.shields.io/github/stars/liliu-avril/Awesome-Segment-Anything?style=social) |
| ğŸ¬ [Awesome-SAM2](https://github.com/GuoleiSun/Awesome-SAM2) | SAM 2 specific resources | ![Stars](https://img.shields.io/github/stars/GuoleiSun/Awesome-SAM2?style=social) |
| ğŸ¥ [SAM4MIS](https://github.com/YichiZhang98/SAM4MIS) | Medical imaging collection | ![Stars](https://img.shields.io/github/stars/YichiZhang98/SAM4MIS?style=social) |

#### ğŸ“‘ Paper Lists
- ğŸ“„ [2024 Paper List](https://github.com/liliu-avril/Awesome-Segment-Anything/blob/main/Paper_List/paper_list_2024.md) - Continuously updated

</details>

---

## ğŸ’¡ Domain-Specific Applications

### ğŸ¥ Medical Imaging (2024-2025)

<table>
<tr>
<td width="50%">

#### ğŸ©º [SAM4MIS](https://github.com/YichiZhang98/SAM4MIS)
**CIBM 2024** | ![Stars](https://img.shields.io/github/stars/YichiZhang98/SAM4MIS?style=social)

**Benchmarks:**
- âœ… 15+ medical benchmarks
- âœ… 26+ different tasks
- ğŸ”¬ Mammography, MRI, CT
- ğŸ‘ï¸ Retinal vessel segmentation
- ğŸ«€ Ultrasound imaging

</td>
<td width="50%">

#### ğŸ§Š [SAM-Med3D](https://link.springer.com/chapter/10.1007/978-3-031-91721-9_4)
**General-purpose 3D Medical Segmentation**

**Dataset:**
- ğŸ“¦ SA-Med3D-140K
- ğŸ¯ 22K 3D images
- ğŸ·ï¸ 143K masks
- ğŸ”¬ Multi-modal (CT, MRI, etc.)
- ğŸ¯ Few-shot 3D prompting

</td>
</tr>
</table>

<details>
<summary><b>ğŸ”¬ More Medical Applications</b></summary>

<br/>

#### ğŸ¯ Key Papers (2024-2025)

| Paper | Venue | Application | Performance |
|-------|-------|-------------|-------------|
| [Interactive 3D Medical Segmentation](https://arxiv.org/html/2408.02635v2) | ArXiv 2024 | Zero-shot 3D CT | State-of-art |
| [SAM 2 for 3D Medical Imaging](https://ai.jmir.org/2025/1/e72109/) | JMIR AI 2025 | Abdominal CT scans | Promising |
| [ProtoSAM-3D](https://pubmed.ncbi.nlm.nih.gov/39919534/) | PubMed | Volumetric imaging | Interactive |
| [AutoProSAM](https://openaccess.thecvf.com/content/WACV2025/papers/Li_AutoProSAM_Automated_Prompting_SAM_for_3D_Multi-Organ_Segmentation_WACV_2025_paper.pdf) | WACV 2025 | Multi-organ 3D | Automated |
| [SAM-Med2D Analysis](https://bmcmedimaging.biomedcentral.com/articles/10.1186/s12880-024-01401-6) | BMC 2024 | 2D Medical images | Improved |

#### ğŸ“– Reviews
- ğŸ“š [Review: SAM for Medical Imaging](https://pubmed.ncbi.nlm.nih.gov/39673905/) - Comprehensive 2025 review

</details>

### ğŸ›°ï¸ Remote Sensing & Agriculture

<details>
<summary><b>ğŸŒ¾ Agricultural Applications</b></summary>

<br/>

#### ğŸ›°ï¸ [SAMGeo](https://cfp.scipy.org/2024/talk/ZPBYF7/)
**SciPy 2024 Presentation**

- ğŸŒ Automated remote sensing segmentation
- ğŸ“¦ Open-source geospatial package
- ğŸ¯ User-friendly API

#### ğŸŒ± [SAM for Crop Mapping](https://www.mdpi.com/2072-4292/16/9/1505)
**MDPI Remote Sensing 2024**

```python
# Automated sample generation
âœ… Sentinel-2 imagery (10m resolution)
âœ… Landsat-8 support (30m resolution)
âœ… Automatic quality filtering
âœ… Sample cleaning pipeline
```

#### ğŸšœ [Can SAM Recognize Crops?](https://arxiv.org/html/2311.15138v2)
**ArXiv 2023** | Zero-shot Performance Evaluation

- ğŸ¯ Crop-type mapping
- ğŸ“Š Precision agriculture
- ğŸ” Zero-shot capabilities
- ğŸŒ¾ Multi-spectral challenges

#### ğŸ—ºï¸ [ESRI ArcGIS Integration](https://www.esri.com/arcgis-blog/products/arcgis-pro/geoai/revolutionizing-image-segmentation-with-sam-segment-anything-model/)

- ğŸ™ï¸ Urban planning
- ğŸŒ² Environmental monitoring
- ğŸ’§ Water body extraction
- ğŸ—ï¸ Infrastructure mapping

</details>

### ğŸš— Autonomous Driving & Robotics

<details>
<summary><b>ğŸ¤– Robotics & AV Applications</b></summary>

<br/>

#### ğŸš™ [SAMUNet](https://www.sciencedirect.com/science/article/abs/pii/S0262885625002914)
**2025** | Shape-aware 3D Object Detection

- ğŸ¯ Pillar-based detection
- ğŸš— Autonomous driving optimized
- ğŸ“Š Enhanced 3D understanding

#### â˜ï¸ Point Cloud Resources

- ğŸ“Š [7 SOTA Point Cloud Models](https://segments.ai/blog/7-state-of-the-art-3d-point-cloud-models-for-autonomous-driving/)
- ğŸ¯ LiDAR-based detection
- ğŸ¤– Robotic perception
- ğŸŒ 3D scene understanding

**Applications:**
```
âœ… 3D Object Detection
âœ… Semantic Segmentation
âœ… Instance Segmentation
âœ… Panoptic Segmentation
```

</details>

### ğŸ­ Industrial Quality Control

<details>
<summary><b>ğŸ” Defect Detection & QC</b></summary>

<br/>

#### âš™ï¸ [Weld Defect Detection with SAM](https://www.mdpi.com/1424-8220/25/1/277)
**MDPI Sensors 2025**

```yaml
Application: Oil & Gas Pipeline Inspection
Method: Ultrasonic B-scan Analysis
Models: SAM 1 (ViT-Base) + SAM 2 (Hiera-Base+)
Performance: F1-Score 0.940
Defect Type: Lack of Fusion (LOF)
```

#### ğŸ”§ [Armature Defect Detection](https://www.mdpi.com/2227-9717/13/2/532)
**MDPI Processes 2025** | YOLO11 + SAM

- ğŸ¯ Micro-vibration motor QC
- ğŸ¤– YOLO11 detection + SAM segmentation
- ğŸ“Š Quantitative severity assessment
- âœ… **90%+ accuracy**
- âš¡ Real-time capable

**Benefits:**
```
âœ… Automated inspection
âœ… Cost-effective solution
âœ… Real-time analysis
âœ… Quantitative metrics
```

</details>

### ğŸ¨ Specialized Domains

<details>
<summary><b>ğŸŒˆ Other Applications</b></summary>

<br/>

| Domain | Project | Description |
|--------|---------|-------------|
| ğŸ“ **Depth** | [Depth-Anything-V2](https://huggingface.co/spaces/depth-anything/Depth-Anything-V2) | Monocular depth estimation |
| ğŸ® **3D Gaussian** | [gaussian-grouping](https://github.com/lkeab/gaussian-grouping) | 3D Gaussian splatting |
| ğŸ—ï¸ **3D Recon** | [garfield](https://github.com/chungmin99/garfield) | 3D reconstruction |
| ğŸ”· **Mesh** | [MeshAnything](https://github.com/buaacyw/MeshAnything) | 3D mesh generation |
| ğŸ¬ **4D** | [SA4D](https://github.com/hustvl/SA4D) | 4D scene understanding |
| ğŸ“ **OCR** | [OCR-SAM](https://github.com/yeungchenwa/OCR-SAM) | Text recognition |
| ğŸ• **Food** | [FOOD-SAM](https://github.com/jamesjg/foodsam) | Food segmentation |
| ğŸ“¸ **Deblur** | [SAM-Deblur](https://github.com/hplqaq/sam-deblur) | Image deblurring |

</details>

---

## ğŸ“ Training & Fine-Tuning

<div align="center">

### ğŸ§¬ Fine-Tuning Frameworks & LoRA Adapters

</div>

<details open>
<summary><b>ğŸ”§ Top Fine-Tuning Repositories</b></summary>

<br/>

<table>
<tr>
<th>Repository</th>
<th>Method</th>
<th>Domain</th>
<th>Updated</th>
</tr>
<tr>
<td><a href="https://github.com/mazurowski-lab/finetune-SAM"><b>finetune-SAM</b></a></td>
<td>LoRA + Full</td>
<td>ğŸ¥ Medical</td>
<td>âœ… 2024</td>
</tr>
<tr>
<td><a href="https://github.com/WangRongsheng/SAM-fine-tune"><b>SAM-fine-tune</b></a> ğŸŒŒ</td>
<td>LoRA</td>
<td>ğŸŒ General</td>
<td>âœ… Active</td>
</tr>
<tr>
<td><a href="https://github.com/michael11albrecht/lora_sam"><b>lora_sam</b></a></td>
<td>LoRA + ğŸ¤—</td>
<td>ğŸ¯ Vision</td>
<td>âœ… 2024</td>
</tr>
<tr>
<td><a href="https://github.com/hitachinsk/SAMed"><b>SAMed</b></a></td>
<td>LoRA</td>
<td>ğŸ¥ Medical</td>
<td>âœ… Stable</td>
</tr>
<tr>
<td><a href="https://github.com/vpulab/med-sam-brain"><b>med-sam-brain</b></a></td>
<td>PEFT + LoRA</td>
<td>ğŸ§  Brain Tumor</td>
<td>âœ… 2024</td>
</tr>
</table>

</details>

<details>
<summary><b>ğŸ“š Tutorials & Learning Resources</b></summary>

<br/>

#### ğŸ¯ Official Tutorials

| Resource | Level | Topics |
|----------|-------|--------|
| [Labellerr: SAM + LoRA](https://www.labellerr.com/blog/sam-fine-tuning-using-lora/) | ğŸŸ¢ Beginner | One-shot learning, ship segmentation |
| [Encord: Fine-Tune Guide](https://encord.com/blog/learn-how-to-fine-tune-the-segment-anything-model-sam/) | ğŸŸ¡ Intermediate | Complete pipeline, best practices |
| [Medium: PEFT for Segmentation](https://medium.com/@nischaydiwan1026/exploring-parameter-efficient-fine-tuning-for-foundation-models-in-image-segmentation-49a7701a012a) | ğŸŸ¡ Intermediate | Parameter-efficient methods |

#### ğŸ“„ Research Papers

- ğŸ”¬ [Conv-LoRA](https://openreview.net/forum?id=ezscMer8L0) - OpenReview
  - Lightweight convolutional parameters
  - Combined with LoRA
  - Enhanced performance

#### ğŸ’¡ Quick Start Example

```python
from transformers import SamModel, SamProcessor
from peft import LoraConfig, get_peft_model

# Load base model
model = SamModel.from_pretrained("facebook/sam-vit-base")

# Configure LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["qkv"],
    lora_dropout=0.05,
)

# Apply LoRA
model = get_peft_model(model, lora_config)

# Fine-tune on your data
# ... training code ...
```

</details>

---

## ğŸ› ï¸ Production Deployment & Tools

### ğŸ·ï¸ Annotation Tools with SAM Integration

<table>
<tr>
<td width="50%">

#### âœ¨ [CVAT](https://www.cvat.ai)
**Computer Vision Annotation Tool**

**SAM 2 Integration (2024):**
- ğŸ¬ Video annotation @ **44 FPS**
- ğŸ¯ Semi-automatic segmentation
- ğŸš€ Real-time object tracking
- ğŸ–±ï¸ Interactive prompting

**Resources:**
- ğŸ“ [SAM 2 Video Annotation](https://www.cvat.ai/resources/blog/video-annotation-sam-2)
- ğŸ–¼ï¸ [SAM 2 Image Segmentation](https://www.cvat.ai/resources/blog/meta-segment-anything-model-v2-is-now-available-in-cvat-ai)

</td>
<td width="50%">

#### ğŸ·ï¸ [Label Studio](https://labelstud.io)
**ML Data Labeling Platform**

**Features:**
- ğŸ¤— HuggingFace Spaces integration
- ğŸŒ Multi-modal (image, video, text, audio)
- ğŸ” SSO & RBAC
- â˜ï¸ Cloud-native ML pipelines
- ğŸ³ Docker/Kubernetes deployment

**Resources:**
- ğŸ“ [SAM in HuggingFace Spaces](https://labelstud.io/blog/segment-anything-in-hugging-face-spaces/)

</td>
</tr>
</table>

<details>
<summary><b>ğŸ“Š CVAT vs Label Studio Comparison</b></summary>

<br/>

| Feature | CVAT | Label Studio |
|---------|------|--------------|
| **Best For** | Video annotation, beginners | Enterprise, multi-modal |
| **SAM Integration** | âœ… SAM 2 native | âœ… Via HuggingFace |
| **Video Tools** | â­â­â­â­â­ | â­â­â­ |
| **Enterprise** | â­â­â­ | â­â­â­â­â­ |
| **Ease of Use** | â­â­â­â­â­ | â­â­â­â­ |
| **ML Pipeline** | â­â­â­ | â­â­â­â­â­ |
| **Pricing** | Free + Enterprise | Free + Enterprise |

</details>

<details>
<summary><b>ğŸ”§ Other Annotation Tools</b></summary>

<br/>

- ğŸ–¥ï¸ [AnyLabeling](https://github.com/vietanhdev/anylabeling) - Desktop labeling with SAM
- ğŸ§‚ [SALT](https://github.com/anuragxel/salt) - Segment Anything Labelling Tool

</details>

### â˜ï¸ Deployment Platforms

<details>
<summary><b>ğŸš€ Deployment Options</b></summary>

<br/>

#### ğŸ¤— HuggingFace Ecosystem

```python
# Install transformers
pip install transformers

# Use SAM with transformers
from transformers import SamModel, SamProcessor

model = SamModel.from_pretrained("facebook/sam-vit-huge")
processor = SamProcessor.from_pretrained("facebook/sam-vit-huge")
```

**Resources:**
- ğŸ“š [Official Docs](https://huggingface.co/docs/transformers/en/model_doc/sam)
- ğŸ¯ [Models Hub](https://huggingface.co/models?other=sam)
- ğŸš€ [Jozu MLOps: HF to Production](https://jozu.com/blog/from-hugging-face-to-production-deploying-segment-anything-sam-with-jozus-model-import-feature/)

#### ğŸ”„ Export & Conversion Tools

| Tool | Format | Features |
|------|--------|----------|
| [SAM2ONNX](https://github.com/DmitryYurov/SAM2ONNX) | ONNX | SAM 2 converter |
| [sam_onnx_full_export](https://github.com/AndreyGermanov/sam_onnx_full_export) | ONNX | Complete export |
| [sam4onnx](https://github.com/PINTO0309/sam4onnx) | ONNX | Optimization |
| [samexporter](https://github.com/vietanhdev/samexporter) | Multi | Multi-format |

</details>

---

## ğŸ”§ SAM Extensions & Variants

### ğŸ¯ Grounding & Multi-Modal

<details open>
<summary><b>ğŸ”¥ Popular Extensions</b></summary>

<br/>

#### ğŸŒŸ [Grounded-Segment-Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything)
![Stars](https://img.shields.io/github/stars/IDEA-Research/Grounded-Segment-Anything?style=social)

**The Ultimate Combo:**
```
ğŸ¯ Grounding DINO (Detection)
    â†“
ğŸ¨ SAM (Segmentation)
    â†“
ğŸ–¼ï¸ Stable Diffusion (Generation)
    â†“
âœ¨ Detect â†’ Segment â†’ Generate ANYTHING
```

**Features:**
- âœ… Text-to-detection
- âœ… Automatic segmentation
- âœ… Image inpainting
- âœ… HuggingFace integration

#### ğŸ›¡ï¸ [RobustSAM](https://github.com/robustsam/RobustSAM)
**CVPR 2024** | Adversarial Robustness

- ğŸ”’ Robust to adversarial attacks
- ğŸ¯ Enhanced generalization
- ğŸ“Š Better performance on corrupted images

#### ğŸ¨ [Personalize-SAM](https://github.com/ZrrSkywalker/Personalize-SAM)
**DreamBooth Integration**

- ğŸ­ Personalized segmentation
- ğŸ¤— [Transformers Tutorial](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/PerSAM)
- ğŸ“Š [vs Mask R-CNN](https://encord.com/blog/mask-rcnn-vs-per-sam/)

</details>

<details>
<summary><b>ğŸ§© Full Pipeline Solutions</b></summary>

<br/>

| Project | Description | Key Feature |
|---------|-------------|-------------|
| [SEEM](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once) | Segment Everything Everywhere | Universal segmentation |
| [Full-SAM](https://github.com/ByungKwanLee/Full-Segment-Anything) | Complete pipeline | End-to-end solution |
| [AUTODISTILL](https://github.com/autodistill/autodistill) | Auto labeling | Dataset generation |
| [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO) | Language grounding | Text-guided detection |
| [RAM](https://github.com/xinyu1205/recognize-anything/tree/main) | Recognize Anything | Image tagging |
| [CLIP](https://github.com/OpenAI/CLIP) | Vision-Language | Foundation model |

</details>

---

## ğŸ’» Implementation Libraries

### ğŸ”¤ Multi-Language Support

<details>
<summary><b>âš™ï¸ C++ Implementations</b></summary>

<br/>

| Repository | Framework | Platform |
|------------|-----------|----------|
| [segment-anything-cpp-wrapper](https://github.com/dinglufe/segment-anything-cpp-wrapper) | Pure C++ | Cross-platform |
| [sam-cpp-macos](https://github.com/ryouchinsa/sam-cpp-macos) | Extended | macOS |
| [sam.cpp](https://github.com/YavorGIvanov/sam.cpp) | GGML | Multi-platform |
| [SegmentAnything-OnnxRunner](https://github.com/OroChippw/SegmentAnything-OnnxRunner) | ONNX | Cross-platform |
| [SAM-ONNX-AX650-CPP](https://github.com/ZHEQIUSHUI/SAM-ONNX-AX650-CPP) | QT + Lama | GUI + Inpaint |

</details>

<details>
<summary><b>ğŸ”· Other Languages</b></summary>

<br/>

#### C#
- ğŸ’ [SamSharp](https://github.com/thislookshard/SamSharp) - C# implementation

#### Rust ğŸ¦€
- âš¡ [sam_onnx_rust](https://github.com/AndreyGermanov/sam_onnx_rust) - Rust ONNX

#### LibTorch ğŸ”¥
- ğŸ¯ [Libtorch-MobileSAM-Example](https://github.com/cyrillkuettel/Libtorch-MobileSAM-Example) - C++ PyTorch

</details>

<details>
<summary><b>ğŸ“± Mobile Runtimes</b></summary>

<br/>

#### MNN Framework
- ğŸ“± [EdgeSAM-MNN](https://github.com/slz929/EdgeSAM-MNN) - EdgeSAM on MNN
- ğŸš€ [mnn-segment-anything](https://github.com/wangzhaode/mnn-segment-anything) - Mobile Neural Network

</details>

<details>
<summary><b>ğŸ¨ Creative Tools Integration</b></summary>

<br/>

#### ComfyUI
- ğŸ­ [comfyui_segment_anything](https://github.com/storyicon/comfyui_segment_anything) - SAM nodes for ComfyUI workflow

</details>

---

## ğŸ“Š Datasets & Benchmarks

<div align="center">

### ğŸ—„ï¸ Official Meta Datasets

</div>

<table>
<tr>
<th width="50%">SA-1B (Images)</th>
<th width="50%">SA-V (Videos)</th>
</tr>
<tr>
<td>

**ğŸ“¦ [SA-1B Dataset](https://ai.meta.com/datasets/segment-anything/)**

```yaml
Images: 11 Million
Masks: 1+ Billion
Type: Open world images
License: Licensed & privacy-respecting
Status: âœ… Released 2023
```

**Superlatives:**
- ğŸ† Largest segmentation dataset
- ğŸŒ Diverse geography
- ğŸ¯ High-quality annotations

</td>
<td>

**ğŸ¬ [SA-V Dataset](https://encord.com/blog/segment-anything-model-2-sam-2/)**

```yaml
Videos: 51,000
Countries: 47
Masks: 600,000+
Resolution: 240p â†’ 4K
Duration: 4s â†’ 138s
Status: âœ… Released 2024
```

**Features:**
- ğŸŒ Global diversity
- ğŸ¥ Multi-resolution
- â±ï¸ Variable length

</td>
</tr>
</table>

### ğŸ¯ Benchmark Datasets

<details>
<summary><b>ğŸ“Š SAM 2 Evaluation Benchmarks</b></summary>

<br/>

| Benchmark | Task | Metrics |
|-----------|------|---------|
| **DAVIS** | Video object segmentation | J&F score |
| **MOSE** | Multi-object segmentation | J&F score |
| **LVOS** | Long-term video segmentation | Success rate |
| **YouTube-VOS** | Large-scale video | J&F score |
| **COCO** | Instance segmentation | AP, AR |
| **LVIS** | Large vocabulary segmentation | AP, APr |

**SAM 2 Performance:**
```
âœ… 3x fewer interactions
âœ… Better accuracy
âœ… 6x faster inference
âœ… SOTA on all benchmarks
```

</details>

---

## ğŸ“– Educational Resources & Tutorials

<div align="center">

### ğŸ“ Learn SAM from Scratch to Production

</div>

### ğŸ“š Comprehensive Guides

<details open>
<summary><b>ğŸŒŸ Must-Read Tutorials</b></summary>

<br/>

<table>
<tr>
<th>Resource</th>
<th>Level</th>
<th>Topics</th>
<th>Link</th>
</tr>
<tr>
<td>ğŸ¯ <b>Encord Ultimate Guide</b></td>
<td>ğŸŸ¢ All</td>
<td>Architecture, Training, Applications</td>
<td><a href="https://encord.com/blog/segment-anything-model-explained/">Read</a></td>
</tr>
<tr>
<td>ğŸ¤– <b>Roboflow Breakdown</b></td>
<td>ğŸŸ¢ Beginner</td>
<td>Concepts, Use Cases</td>
<td><a href="https://blog.roboflow.com/segment-anything-breakdown/">Read</a></td>
</tr>
<tr>
<td>ğŸ› ï¸ <b>Roboflow How-to</b></td>
<td>ğŸŸ¡ Intermediate</td>
<td>Practical Implementation</td>
<td><a href="https://blog.roboflow.com/how-to-use-segment-anything-model-sam/">Read</a></td>
</tr>
<tr>
<td>ğŸ·ï¸ <b>V7 Labs Guide</b></td>
<td>ğŸŸ¢ All</td>
<td>Complete Overview</td>
<td><a href="https://www.v7labs.com/blog/segment-anything-model-sam">Read</a></td>
</tr>
<tr>
<td>ğŸ‘¨â€ğŸ’» <b>LabelVisor Hands-on</b></td>
<td>ğŸŸ¡ Intermediate</td>
<td>Effortless Segmentation</td>
<td><a href="https://www.labelvisor.com/effortless-segmentation-in-seconds-a-hands-on-guide-to-using-segment-anything/">Read</a></td>
</tr>
</table>

</details>

### âš¡ Performance & Optimization

<details>
<summary><b>ğŸš€ Optimization Deep Dives</b></summary>

<br/>

#### ğŸ“Š Comparison Articles

- ğŸ“ˆ [EfficientSAM vs SAM](https://medium.com/@nandinilreddy/efficient-sam-vs-sam-920879408467) - Detailed comparison
- ğŸƒ [Fast Faster SAM](https://someshfengde.medium.com/revolutionizing-object-segmentation-fast-faster-sam-673022615849) - Speed optimization
- ğŸ“± [Faster SAM for Mobile](https://ar5iv.labs.arxiv.org/html/2306.14289) - Mobile deployment
- ğŸ” [SAM and Friends](https://www.lightly.ai/blog/segment-anything-model-and-friends) - Model ecosystem

</details>

### ğŸ¥ Video Resources

<details>
<summary><b>ğŸ“º YouTube Channels</b></summary>

<br/>

| Channel | Focus | Subscriber Count |
|---------|-------|------------------|
| [Rob Mulla](https://www.youtube.com/@robmulla) | ML Tutorials, SAM Applications | Data Science |
| [ArjanCodes](https://www.youtube.com/@ArjanCodes/videos) | Software Engineering, Clean Code | Python & AI |

</details>

---

## ğŸ¯ Key Insights & Trends (2024-2025)

<div align="center">

### ğŸš€ Major Developments Timeline

</div>

```mermaid
timeline
    title SAM Evolution 2024-2025
    2024-07 : SAM 2 Release
           : Video Segmentation
    2024-09 : SAM 2.1 Update
           : ICLR 2025 Award
    2024-12 : Medical Breakthrough
           : 26+ Tasks, 15+ Benchmarks
    2025-02 : AWS Integration
           : SageMaker JumpStart
```

### ğŸ“Š Performance Benchmarks

<table>
<tr>
<th>Metric</th>
<th>Value</th>
<th>Model</th>
</tr>
<tr>
<td>âš¡ <b>Speed Improvement</b></td>
<td><b>6x faster</b></td>
<td>SAM 2 vs SAM 1</td>
</tr>
<tr>
<td>ğŸ¯ <b>Efficiency Gain</b></td>
<td><b>3x fewer interactions</b></td>
<td>SAM 2</td>
</tr>
<tr>
<td>ğŸ“± <b>Mobile FPS</b></td>
<td><b>30+ FPS</b></td>
<td>EdgeSAM (iPhone 14)</td>
</tr>
<tr>
<td>ğŸ¬ <b>Video FPS</b></td>
<td><b>44 FPS</b></td>
<td>SAM 2 real-time</td>
</tr>
<tr>
<td>ğŸ¥ <b>Surgical FPS</b></td>
<td><b>86 FPS</b></td>
<td>Surgical SAM 2</td>
</tr>
<tr>
<td>ğŸ­ <b>Industrial Accuracy</b></td>
<td><b>F1: 0.940</b></td>
<td>Weld Defect Detection</td>
</tr>
<tr>
<td>ğŸ“ <b>Medical Tasks</b></td>
<td><b>26+ tasks</b></td>
<td>SAM4MIS</td>
</tr>
</table>

### ğŸ”¬ Research Trends

<details>
<summary><b>ğŸŒŸ Emerging Research Directions</b></summary>

<br/>

#### ğŸ¯ Key Trends 2024-2025

```
1. ğŸš€ Training-Free Adaptation
   â””â”€ LoRA, Conv-LoRA, Prompt Tuning

2. ğŸ¬ Video Understanding
   â””â”€ SAM 2, SAM2Long, Temporal Consistency

3. ğŸ¥ Medical Imaging
   â””â”€ 3D Segmentation, Multi-modal Fusion

4. ğŸ“± Edge Deployment
   â””â”€ Quantization, Pruning, Distillation

5. ğŸŒ Multi-Modal Integration
   â””â”€ Audio-Visual, Language-Vision

6. ğŸ¯ Zero-Shot Transfer
   â””â”€ Cross-domain, Few-shot Learning

7. ğŸ® 3D/4D Understanding
   â””â”€ Point Clouds, Temporal Dynamics
```

#### ğŸ“ˆ Growth Areas

| Domain | Growth | Key Applications |
|--------|--------|------------------|
| ğŸ¥ Medical | â­â­â­â­â­ | 3D imaging, Surgery |
| ğŸš— Autonomous | â­â­â­â­ | LiDAR, Perception |
| ğŸ›°ï¸ Remote Sensing | â­â­â­â­ | Agriculture, Mapping |
| ğŸ­ Industrial | â­â­â­â­â­ | QC, Defect Detection |
| ğŸ¨ Creative | â­â­â­ | Editing, Generation |

</details>

---

## ğŸ¨ "Anything" Projects Ecosystem

### ğŸ’« 2024 Innovative Projects

<details>
<summary><b>ğŸŒŸ Click to explore 2024 projects</b></summary>

<br/>

| Project | Description | Stars |
|---------|-------------|-------|
| [ReplaceAnything](https://github.com/AIGCDesignGroup/ReplaceAnything) | ğŸ­ Replace objects in images | ![Stars](https://img.shields.io/github/stars/AIGCDesignGroup/ReplaceAnything?style=social) |
| [Depth-Anything](https://github.com/LiheYoung/Depth-Anything) | ğŸ“ Monocular depth estimation | ![Stars](https://img.shields.io/github/stars/LiheYoung/Depth-Anything?style=social) |
| [OMG-Seg](https://github.com/lxtgh/omg-seg) | ğŸŒ Open-world segmentation | ![Stars](https://img.shields.io/github/stars/lxtgh/omg-seg?style=social) |
| [OVSAM](https://github.com/harboryuan/ovsam) | ğŸ“– Open-vocabulary SAM | ![Stars](https://img.shields.io/github/stars/harboryuan/ovsam?style=social) |

</details>

### ğŸ¯ 2023 Classic Projects

<details>
<summary><b>ğŸ“š Essential 2023 projects</b></summary>

<br/>

#### ğŸ¨ Creative & Editing
- [Matting-Anything](https://github.com/shi-labs/matting-anything) - Professional image matting
- [Inpaint-Anything](https://github.com/geekyutao/Inpaint-Anything) - Intelligent inpainting
- [EditAnything](https://github.com/sail-sg/EditAnything) - Flexible image editing
- [Painter](https://github.com/baaivision/Painter) - Generative models

#### ğŸ” Detection & Segmentation
- [Prompt-Segment-Anything](https://github.com/RockeyCoss/Prompt-Segment-Anything) - Advanced prompting
- [Semantic-Segment-Anything](https://github.com/fudan-zvg/Semantic-Segment-Anything) - Semantic seg
- [Segment-Any-Anomaly](https://github.com/caoyunkang/Segment-Any-Anomaly) - Anomaly detection
- [GroundedSAM-Anomaly](https://github.com/caoyunkang/GroundedSAM-zero-shot-anomaly-detection) - Zero-shot AD

#### ğŸ“Š Analysis & Understanding
- [Caption-Anything](https://github.com/ttengwang/Caption-Anything) - Image captioning
- [Count-Anything](https://github.com/ylqi/Count-Anything) - Object counting
- [detect-anyshadow](https://github.com/harrytea/detect-anyshadow) - Shadow detection
- [ShowAnything](https://github.com/showlab/ShowAnything) - Visualization

#### ğŸ® 3D & Reconstruction
- [Anything-3D](https://github.com/Anything-of-anything/Anything-3D) - 3D reconstruction

</details>

---

## ğŸ¤ Contributing

<div align="center">

### ğŸ’¡ Help Us Grow This Collection!

[![Contributors](https://img.shields.io/badge/Contributors-Welcome-brightgreen?style=for-the-badge)](https://github.com)
[![PRs](https://img.shields.io/badge/PRs-Welcome-blue?style=for-the-badge)](https://github.com)

</div>

**We welcome contributions!** Please ensure:

- âœ… Resources are from reputable sources
- âœ… Links are active and high-quality
- âœ… Descriptions are accurate and concise
- âœ… Proper categorization
- âœ… Include relevant badges/stars
- âœ… Add performance metrics when available

---

## ğŸ“„ Citation

<details>
<summary><b>ğŸ“ BibTeX Citations</b></summary>

<br/>

### SAM (Original)
```bibtex
@article{kirillov2023segment,
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={arXiv:2304.02643},
  year={2023}
}
```

### SAM 2
```bibtex
@article{ravi2024sam2,
  title={SAM 2: Segment Anything in Images and Videos},
  author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Doll{\'a}r, Piotr and Feichtenhofer, Christoph},
  journal={arXiv:2408.00714},
  year={2024}
}
```

</details>

---

<div align="center">

## ğŸŒŸ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=umitkacar/SAM-Foundation-Models&type=Date)](https://star-history.com/#umitkacar/SAM-Foundation-Models&Date)

---

### ğŸ“Š Repository Stats

![Last Updated](https://img.shields.io/badge/Last%20Updated-January%202025-blue?style=flat-square)
![Maintenance](https://img.shields.io/badge/Maintained-Yes-green?style=flat-square)
![Resources](https://img.shields.io/badge/Resources-100+-red?style=flat-square)

---

**Last Updated:** January 2025 ğŸ—“ï¸
**Maintainer:** Community-driven ğŸ‘¥
**License:** Collection of resources with individual licenses ğŸ“œ

---

<sub>**Disclaimer:** This is a curated collection. Each project has its own license.</sub>

### â­ Star this repo to stay updated with the latest SAM developments!
### ğŸ‘€ Watch for new resources and updates!

<br/>

**Made with â¤ï¸ by the SAM Community**

</div>
